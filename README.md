# BrainTestStudio - Workflow Dashboard

Ein responsives React-Dashboard fÃ¼r Workflow-Management, Feedback und Pipeline-Ãœbersichten.

## ğŸš€ Features

- **Responsive Design**: Passt sich automatisch an verschiedene BildschirmgrÃ¶ÃŸen an
- **Deutsche BenutzeroberflÃ¤che**: VollstÃ¤ndig in Deutsch
- **Modulare Komponenten**: Einfach erweiterbar fÃ¼r zukÃ¼nftige Features
- **N8N Integration**: Workflow-Automatisierung mit Webhooks
- **Supabase Backend**: PostgreSQL Datenbank mit Auth und Realtime
- **Docker-basiert**: Einfaches Deployment und portables Setup

## ğŸ³ Schnellstart mit Docker

### Voraussetzungen
- Docker & Docker Compose installiert
- Git

### Installation

```bash
# Repository klonen
git clone https://github.com/Unknowren/Dashbaord.git
cd Dashbaord

# Umgebungsvariablen konfigurieren
cp .env.example .env
# Bearbeite .env mit deinen Einstellungen

# Starten
./start.sh
```

### Zugriff auf Services

| Service | URL | Beschreibung |
|---------|-----|--------------|
| Frontend | http://localhost:3000 | React Dashboard |
| N8N | http://localhost:5678 | Workflow Automation |
| Supabase Studio | http://localhost:3001 | Datenbank Admin |
| Supabase API | http://localhost:8000 | REST API |
| Ollama | http://localhost:11434 | Local LLM Engine |

### Stoppen

```bash
./stop.sh
```

## ğŸ“ Projektstruktur

```text
â”œâ”€â”€ src/                    # React Frontend
â”‚   â”œâ”€â”€ components/         # UI Komponenten
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docker/                 # Docker Konfigurationen
â”‚   â”œâ”€â”€ nginx/              # Nginx Webserver Config
â”‚   â”œâ”€â”€ postgres/           # PostgreSQL Init Scripts
â”‚   â””â”€â”€ kong/               # Kong API Gateway Config
â”œâ”€â”€ docker-data/            # Persistente Daten (nicht in Git!)
â”œâ”€â”€ docker-compose.yml      # Docker Compose Konfiguration
â”œâ”€â”€ Dockerfile              # Frontend Container Build
â”œâ”€â”€ .env.example            # Beispiel-Umgebungsvariablen
â””â”€â”€ .env                    # Lokale Umgebungsvariablen (nicht in Git!)
```

## ğŸ·ï¸ Bereichs-IDs fÃ¼r Prompts

FÃ¼r zukÃ¼nftige Anpassungen kÃ¶nnen Sie sich auf folgende Bereiche beziehen:

| Bereich | ID | Beschreibung |
|---------|-----|--------------|
| Header | `#header-bereich` | Obere Leiste mit Suche |
| Hamburger-MenÃ¼ | `#hamburger-menu` | MenÃ¼-Button oben links |
| Header-Suche | `#header-suche` | Suchfeld im Header |
| Sidebar | `#sidebar-navigation` | Linke Navigationsleiste |
| Hauptbereich | `#hauptbereich` | Zentraler Inhaltsbereich |
| Content-Platzhalter | `#content-platzhalter` | Platzhalter fÃ¼r Inhalte |

## ğŸ› ï¸ Lokale Entwicklung (ohne Docker)

```bash
npm install
npm run dev
```

## ğŸ”® Geplante Features

- [x] Docker-basiertes Deployment
- [x] N8N Workflow Integration
- [x] Supabase Datenbank
- [x] Ollama Local LLM Integration
- [ ] Live-Formulare mit Echtzeit-Feedback
- [ ] Workflow-Pipeline-Ãœbersicht
- [ ] Benutzerauthentifizierung

## ğŸ¤– Ollama LLM Integration

Ollama ermÃ¶glicht die lokale Nutzung von groÃŸen Sprachmodellen ohne externe APIs. Die Modelle werden persistent in `./docker-data/ollama` gespeichert.

### Modelle verwalten

**VerfÃ¼gbare Modelle anzeigen:**
```bash
docker exec brainstudio-ollama ollama list
```

**Neues Modell laden:**
```bash
# Mistral (klein, schnell, ~4GB)
docker exec brainstudio-ollama ollama pull mistral

# Llama 2 (grÃ¶ÃŸer, besser, ~7GB)
docker exec brainstudio-ollama ollama pull llama2

# Neural Chat (spezialisiert fÃ¼r Chat, ~5GB)
docker exec brainstudio-ollama ollama pull neural-chat

# OpenChat (leicht, ~4GB)
docker exec brainstudio-ollama ollama pull openchat
```

**Modell testen (Streaming Response):**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "ErklÃ¤re KÃ¼nstliche Intelligenz in 3 SÃ¤tzen",
  "stream": true
}' | jq '.'
```

**Modell lÃ¶schen:**
```bash
docker exec brainstudio-ollama ollama rm mistral
```

**Modell-Speichernutzung prÃ¼fen:**
```bash
ls -lh ./docker-data/ollama/models/manifests/registry.ollama.ai/library/
```

### Integration mit N8N

In N8N Workflows kannst du Ollama nutzen:

```json
{
  "method": "POST",
  "url": "http://ollama:11434/api/generate",
  "body": {
    "model": "mistral",
    "prompt": "{{ $node.PreviousNode.json.input }}",
    "stream": false
  }
}
```

### Performance-Tipps

- **Kleine Modelle** (mistral, neural-chat): Schneller, weniger Speicher
- **GroÃŸe Modelle** (llama2 13b): Bessere QualitÃ¤t, lÃ¤nger Rechenzeit
- **GPU-Beschleunigung** auf macOS/Windows: Optional, Ollama nutzt CPU wenn kein GPU-Support

## ğŸ¨ Design

Das Dashboard verwendet ein graues Farbschema basierend auf dem ursprÃ¼nglichen Entwurf:

- Header: Dunkelgrau (#8a8a8a)
- Sidebar: Mittelgrau (#9a9a9a)
- Hauptbereich: WeiÃŸ (#ffffff)

## âš ï¸ Wichtige Hinweise

- **`.env` niemals committen!** EnthÃ¤lt sensible Zugangsdaten
- **`docker-data/`** enthÃ¤lt persistente Daten und wird nicht committed
- FÃ¼r Kollegen: `.env.example` kopieren und eigene Werte eintragen

## ğŸ“ Lizenz

Privates Projekt
