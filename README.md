# BrainTestStudio - Workflow Dashboard

Ein responsives React-Dashboard fÃ¼r Workflow-Management, Feedback und Pipeline-Ãœbersichten.

## ğŸš€ Features

- **Responsive Design**: Passt sich automatisch an verschiedene BildschirmgrÃ¶ÃŸen an
- **Deutsche BenutzeroberflÃ¤che**: VollstÃ¤ndig in Deutsch
- **Modulare Komponenten**: Einfach erweiterbar fÃ¼r zukÃ¼nftige Features
- **N8N Integration**: Workflow-Automatisierung mit Webhooks
- **Supabase Backend**: PostgreSQL Datenbank mit Auth und Realtime
- **Docker-basiert**: Einfaches Deployment und portables Setup

## ğŸ³ Schnellstart mit Docker

### Voraussetzungen
- Docker & Docker Compose installiert
- Git

### Installation

```bash
# Repository klonen
git clone https://github.com/Unknowren/Dashbaord.git
cd Dashbaord

# Umgebungsvariablen konfigurieren
cp .env.example .env
# Bearbeite .env mit deinen Einstellungen

# Starten
./start.sh
```

### Zugriff auf Services

| Service | URL | Beschreibung |
|---------|-----|--------------|
| Frontend | http://localhost:3000 | React Dashboard |
| N8N | http://localhost:5678 | Workflow Automation |
| Supabase Studio | http://localhost:3001 | Datenbank Admin |
| Supabase API | http://localhost:8000 | REST API |
| Ollama | http://localhost:11434 | Local LLM Engine |
| Zammad | http://localhost:8080 | Helpdesk & Ticketsystem |

### Stoppen

```bash
./stop.sh
```

## ğŸ“ Projektstruktur

```text
â”œâ”€â”€ src/                    # React Frontend
â”‚   â”œâ”€â”€ components/         # UI Komponenten
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docker/                 # Docker Konfigurationen
â”‚   â”œâ”€â”€ nginx/              # Nginx Webserver Config
â”‚   â”œâ”€â”€ postgres/           # PostgreSQL Init Scripts
â”‚   â””â”€â”€ kong/               # Kong API Gateway Config
â”œâ”€â”€ docker-data/            # Persistente Daten (nicht in Git!)
â”œâ”€â”€ docker-compose.yml      # Docker Compose Konfiguration
â”œâ”€â”€ Dockerfile              # Frontend Container Build
â”œâ”€â”€ .env.example            # Beispiel-Umgebungsvariablen
â””â”€â”€ .env                    # Lokale Umgebungsvariablen (nicht in Git!)
```

## ğŸ·ï¸ Bereichs-IDs fÃ¼r Prompts

FÃ¼r zukÃ¼nftige Anpassungen kÃ¶nnen Sie sich auf folgende Bereiche beziehen:

| Bereich | ID | Beschreibung |
|---------|-----|--------------|
| Header | `#header-bereich` | Obere Leiste mit Suche |
| Hamburger-MenÃ¼ | `#hamburger-menu` | MenÃ¼-Button oben links |
| Header-Suche | `#header-suche` | Suchfeld im Header |
| Sidebar | `#sidebar-navigation` | Linke Navigationsleiste |
| Hauptbereich | `#hauptbereich` | Zentraler Inhaltsbereich |
| Content-Platzhalter | `#content-platzhalter` | Platzhalter fÃ¼r Inhalte |

## ğŸ› ï¸ Lokale Entwicklung (ohne Docker)

```bash
npm install
npm run dev
```

## ğŸ”® Geplante Features

- [x] Docker-basiertes Deployment
- [x] N8N Workflow Integration
- [x] Supabase Datenbank
- [x] Ollama Local LLM Integration
- [x] Zammad Helpdesk Integration
- [ ] Live-Formulare mit Echtzeit-Feedback
- [ ] Workflow-Pipeline-Ãœbersicht
- [ ] Benutzerauthentifizierung

## ğŸ¤– Ollama LLM Integration

Ollama ermÃ¶glicht die lokale Nutzung von groÃŸen Sprachmodellen ohne externe APIs. Die Modelle werden persistent in `./docker-data/ollama` gespeichert.

### Modelle verwalten

**VerfÃ¼gbare Modelle anzeigen:**
```bash
docker exec brainstudio-ollama ollama list
```

**Neues Modell laden:**
```bash
# Mistral (klein, schnell, ~4GB)
docker exec brainstudio-ollama ollama pull mistral

# Llama 2 (grÃ¶ÃŸer, besser, ~7GB)
docker exec brainstudio-ollama ollama pull llama2

# Neural Chat (spezialisiert fÃ¼r Chat, ~5GB)
docker exec brainstudio-ollama ollama pull neural-chat

# OpenChat (leicht, ~4GB)
docker exec brainstudio-ollama ollama pull openchat
```

**Modell testen (Streaming Response):**
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "ErklÃ¤re KÃ¼nstliche Intelligenz in 3 SÃ¤tzen",
  "stream": true
}' | jq '.'
```

**Modell lÃ¶schen:**
```bash
docker exec brainstudio-ollama ollama rm mistral
```

**Modell-Speichernutzung prÃ¼fen:**
```bash
ls -lh ./docker-data/ollama/models/manifests/registry.ollama.ai/library/
```

### Integration mit N8N

In N8N kannst du Ollama Ã¼ber einen **HTTP Request Node** nutzen:

1. **Neuen HTTP Request Node erstellen**
2. **Konfiguration:**
   - Method: `POST`
   - URL: `http://ollama:11434/api/generate`

3. **Body (JSON):**

```json
{
  "model": "mistral",
  "prompt": "{{ $json.input }}",
  "stream": false
}
```

### Performance-Tipps

- **Kleine Modelle** (mistral, neural-chat): Schneller, weniger Speicher
- **GroÃŸe Modelle** (llama2 13b): Bessere QualitÃ¤t, lÃ¤nger Rechenzeit
- **GPU-Beschleunigung** auf macOS/Windows: Optional, Ollama nutzt CPU wenn kein GPU-Support

## ğŸ« Zammad Helpdesk Integration

Zammad ist ein Open-Source-Helpdesk und Ticketsystem fÃ¼r Kundensupport und interne Anfragen.

### Erster Start

Beim ersten Start von Zammad dauert die Initialisierung einige Minuten. Zammad richtet automatisch die Datenbank ein und erstellt alle benÃ¶tigten Tabellen.

**Status prÃ¼fen:**
```bash
docker logs -f brainstudio-zammad-init
```

**Warten bis die Initialisierung abgeschlossen ist, dann:**
```bash
docker compose up -d zammad-nginx
```

### Zammad Setup-Wizard

Nach dem ersten Start erreichst du den Setup-Wizard unter http://localhost:8080:

1. **Administrator erstellen**: E-Mail und Passwort festlegen
2. **Organisation konfigurieren**: Firmenname und Details eingeben
3. **E-Mail-KanÃ¤le einrichten**: SMTP/IMAP fÃ¼r E-Mail-Tickets (optional)

### Zammad Container

Zammad besteht aus mehreren Services:

| Container | Funktion |
|-----------|----------|
| zammad-nginx | Webserver (Port 8080) |
| zammad-railsserver | Rails Application Server |
| zammad-websocket | WebSocket Server fÃ¼r Echtzeit-Updates |
| zammad-scheduler | Hintergrund-Jobs |
| zammad-postgresql | Zammad-Datenbank |
| zammad-elasticsearch | Volltextsuche |
| zammad-redis | Cache & Sessions |
| zammad-memcached | Object Caching |

### Integration mit N8N

Du kannst Zammad mit N8N Ã¼ber die REST API integrieren:

**API-Token erstellen:**
1. In Zammad einloggen â†’ Admin-Bereich
2. System â†’ API â†’ Token erstellen

**N8N HTTP Request Node:**
- Method: `GET`
- URL: `http://zammad-nginx:8080/api/v1/tickets`
- Header: `Authorization: Token token=DEIN_API_TOKEN`

### Zammad neu starten

```bash
docker compose restart zammad-nginx zammad-railsserver zammad-websocket zammad-scheduler
```

### Zammad Logs anzeigen

```bash
docker logs -f brainstudio-zammad-railsserver
```

## ğŸ¨ Design

Das Dashboard verwendet ein graues Farbschema basierend auf dem ursprÃ¼nglichen Entwurf:

- Header: Dunkelgrau (#8a8a8a)
- Sidebar: Mittelgrau (#9a9a9a)
- Hauptbereich: WeiÃŸ (#ffffff)

## âš ï¸ Wichtige Hinweise

- **`.env` niemals committen!** EnthÃ¤lt sensible Zugangsdaten
- **`docker-data/`** enthÃ¤lt persistente Daten und wird nicht committed
- FÃ¼r Kollegen: `.env.example` kopieren und eigene Werte eintragen

## ğŸ“ Lizenz

Privates Projekt
